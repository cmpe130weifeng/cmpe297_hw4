# cmpe297_hw4 - Implement a colab of meta-learning on top of BERT </br>
</br>
BERT(Bidirectional Encoder Representation from Transformers) is one of the transformers, which provides encoding and decoding for data. BERT often used in NLP and CV. BERT is a pre-trained model, therefore, we can apply it to MTL & meta learning. </br>
</br>
The Bert model I used is bert-base-uncased. It is trained with tons of data. It can provide high performance on sentiment analysis. When we apply Meta-learning, model can easily learn its optimal weights. 
</br>
</br>
Colab Link: https://colab.research.google.com/drive/1rV_IBaDPnasQ_-ewAW6HdQMnEUrYkQXM#scrollTo=vIiPSPMfk1Me
</br>
</br>
Ref:</br>
https://github.com/google-research/bert </br>
https://github.com/mailong25/meta-learning-bert
